# -*- coding: utf-8 -*-
# black: skip file
"""
T5Transformer.py

GENERATED BY METAPROJECT.
"""
# import os
import unittest
import torch

from unittest.mock import MagicMock, patch

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

from llm.extension.t5_transformer import T5Transformer
from llm.extension.base_transformer import ThinkResult

from tests.extension import shared_test_methods
from tests.extension.base_test_class import BaseTestClass


class TestT5Transformer(BaseTestClass):
    """
    Test cases for T5Transformer class.
    """

    def test_initialize(self):
        """
        Test the initialization of the T5Transformer class.

        NOTE: this is already done in generated model. Start by writing
        better test for this class.
        """
        obj = T5Transformer()
        self.assertTrue(obj)
        self.assertEqual(obj.transformer_model_name, "t5-base")

    @patch("llm.extension.t5_transformer.AutoModelForSeq2SeqLM.from_pretrained")
    @patch("llm.extension.t5_transformer.AutoTokenizer.from_pretrained")
    def test_should_generate(
        self, mock_tokenizer_from_pretrained, mock_model_from_pretrained
    ):
        """
        Test the generate method of T5Transformer class.
        """

        # Create a mock tokenizer
        mock_tokenizer = MagicMock()
        mock_tokenizer.return_value = {"input_ids": torch.tensor([[1, 2, 3]])}
        mock_tokenizer_from_pretrained.return_value = mock_tokenizer

        # Create a mock model instance
        mock_model = MagicMock()
        mock_model.generate.return_value = torch.tensor([[1, 2, 3, 4, 5]])
        mock_model_from_pretrained.return_value = mock_model

        obj = T5Transformer()
        result = obj.generate("Test input")

        # Verify the result is a tensor
        self.assertIsInstance(result, torch.Tensor)

        # Verify the model's generate method was called
        mock_model.generate.assert_called_once()

        # Verify tokenizer was called with the input text
        mock_tokenizer.assert_called_once_with("Test input", return_tensors="pt")

    @patch("llm.extension.t5_transformer.AutoModelForSeq2SeqLM.from_pretrained")
    @patch("llm.extension.t5_transformer.AutoTokenizer.from_pretrained")
    def test_should_think(
        self, mock_tokenizer_from_pretrained, mock_model_from_pretrained
    ):
        """
        Test the think method of T5Transformer class.
        """
        # Create a mock tokenizer
        mock_tokenizer = MagicMock()
        mock_tokenizer.return_value = {"input_ids": torch.tensor([[1, 2, 3]])}
        mock_tokenizer.decode.return_value = "Bonjour le monde"
        mock_tokenizer_from_pretrained.return_value = mock_tokenizer

        # Create a mock model instance
        mock_model = MagicMock()
        mock_model.generate.return_value = torch.tensor([[4, 5, 6, 7, 8]])
        mock_model_from_pretrained.return_value = mock_model

        obj = T5Transformer()
        result = obj.think(
            "Translate English to French: Hello world",
            generation_kwargs={"max_length": 50},
        )

        # Verify the result is a string
        self.assertIsInstance(result, ThinkResult)
        self.assertEqual(result.value, "Bonjour le monde")

        # Verify the decode method was called
        mock_tokenizer.decode.assert_called_once()

    @patch("llm.extension.t5_transformer.AutoModelForSeq2SeqLM.from_pretrained")
    @patch("llm.extension.t5_transformer.AutoTokenizer.from_pretrained")
    def test_should_think_multiple(
        self, mock_tokenizer_from_pretrained, mock_model_from_pretrained
    ):
        """
        Test the think method of T5Transformer class with multiple inputs.
        """
        # Create a mock tokenizer
        mock_tokenizer = MagicMock()
        mock_tokenizer.return_value = {"input_ids": torch.tensor([[1, 2, 3]])}
        mock_tokenizer.decode.return_value = "Bonjour le monde"
        mock_tokenizer_from_pretrained.return_value = mock_tokenizer

        # Create a mock model instance
        mock_model = MagicMock()
        mock_model.generate.return_value = torch.tensor([[4, 5, 6, 7, 8]])
        mock_model_from_pretrained.return_value = mock_model

        obj = T5Transformer()
        result = obj.think(
            "Translate English to French: Hello world",
            generate_type="multiple",
        )

        # Verify the result is a string
        self.assertIsInstance(result, ThinkResult)
        self.assertEqual(result.value, ["Bonjour le monde"])

        # Verify the decode method was called
        mock_tokenizer.decode.assert_called_once()

    @patch("llm.extension.t5_transformer.AutoModelForSeq2SeqLM.from_pretrained")
    @patch("llm.extension.t5_transformer.AutoTokenizer.from_pretrained")
    def test_should_think_batch(
        self, mock_tokenizer_from_pretrained, mock_model_from_pretrained
    ):
        """
        Test the think method of T5Transformer class with batch inputs.
        """
        # Create a mock tokenizer
        mock_tokenizer = MagicMock()
        mock_tokenizer.return_value = {"input_ids": torch.tensor([[1, 2, 3]])}
        mock_tokenizer.decode.return_value = "Bonjour le monde"
        mock_tokenizer_from_pretrained.return_value = mock_tokenizer

        # Create a mock model instance
        mock_model = MagicMock()
        mock_model.generate.return_value = torch.tensor([[4, 5, 6, 7, 8]])
        mock_model_from_pretrained.return_value = mock_model

        obj = T5Transformer()
        result = obj.think(
            [
                "Translate English to French: Hello world",
                "Translate English to Spanish: Hello world",
            ],
            generate_type="batch",
            generation_kwargs={"max_length": 50},
        )

        # Verify the result is a string
        self.assertIsInstance(result, ThinkResult)
        self.assertEqual(result.value, ["Bonjour le monde"])

        # Verify the decode method was called
        mock_tokenizer.decode.assert_called_once()

    @patch("llm.extension.t5_transformer.AutoModelForSeq2SeqLM.from_pretrained")
    @patch("llm.extension.t5_transformer.AutoTokenizer.from_pretrained")
    def test_should_think_with_scores(
        self, mock_tokenizer_from_pretrained, mock_model_from_pretrained
    ):
        """
        Test the think method of T5Transformer class with scores.
        """
        # Create a mock tokenizer
        mock_tokenizer = MagicMock()
        mock_tokenizer.return_value = {"input_ids": torch.tensor([[1, 2, 3]])}
        mock_tokenizer.decode.return_value = "Bonjour le monde"
        mock_tokenizer_from_pretrained.return_value = mock_tokenizer

        # Create a mock model instance
        mock_model = MagicMock()
        mock_model.generate.return_value = torch.tensor([[4, 5, 6, 7, 8]])
        mock_model_from_pretrained.return_value = mock_model

        obj = T5Transformer()
        result = obj.think(
            "Translate English to French: Hello world",
            generate_type="with_scores",
            generation_kwargs={"max_length": 50},
        )

        # Verify the result is a string
        self.assertIsInstance(result, ThinkResult)
        self.assertEqual(
            result.value,
            {
                "text": "Bonjour le monde",
                "sequences": mock_model.generate.return_value,
            },
        )

        # Verify the decode method was called
        mock_tokenizer.decode.assert_called_once()


if __name__ == "__main__":
    unittest.main()
